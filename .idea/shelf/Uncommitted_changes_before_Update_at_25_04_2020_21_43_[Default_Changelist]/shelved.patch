Index: data_processing/data_processing/cleansing_datafeed/size_sorter.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import natsort\n\n\nclass SizeSorter:\n    def __init__(self, size_list: list):\n        self.size_list = size_list\n        self.sorting_type: str = self.sorting_type_finder()\n\n    def sorting_type_finder(self) -> str:\n        letter_counter: int = 0\n        number_counter: int = 0\n        average_letter: float = 0\n        average_number: float = 0\n        character_counter: int = 0\n\n        if len(self.size_list) == 1:\n            return \"one_size\"\n        for size in self.size_list:\n\n            for ch in size:\n                character_counter += 1\n                if ch.isdigit():\n                    number_counter += 1\n                if ch.isalpha():\n                    letter_counter += 1\n            if character_counter > 0:\n                average_letter = letter_counter / character_counter\n                average_number = number_counter / character_counter\n            if average_letter > average_number:\n                return \"letter\"\n            else:\n                return \"number\"\n\n    def sort_list(self):\n        if \"OneSize\" or \"NoSize\" in self.size_list:\n            return self.size_list\n        if self.sorting_type == \"one_size\":\n            ordered_sizes = self.size_list\n            return ordered_sizes\n        if self.sorting_type == \"number\":\n            ordered_sizes = natsort.natsorted(self.size_list)\n            return ordered_sizes\n        if self.sorting_type == \"letter\":\n            order_size: dict = {\"XXXXXS\": 0,\n                                \"XXXXS\": 1,\n                                \"XXXS\": 2,\n                                \"XXS\": 3,\n                                \"XS\": 4,\n                                \"S\": 5,\n                                \"M\": 6, \"L\": 7,\n                                \"XL\": 8, \"XXL\": 9, \"XXXL\": 10, \"XXXXL\": 11, \"XXXXXL\": 12,\n                                \"XXXXXXL\":13,\n                                \"XXXXXXXL\":14}\n            conversion_size: dict = {\"5XS\": \"XXXXXS\",\n                                     \"4XS\": \"XXXXS\",\n                                     \"3XS\": \"XXXS\",\n                                     \"2XS\": \"XXS\",\n                                     \"XS\": \"XS\",\n                                     \"S\": \"S\",\n                                     \"M\": \"M\",\n                                     \"L\": \"L\",\n                                     \"XL\": \"XL\",\n                                     \"2XL\": \"XXL\",\n                                     \"3XL\": \"XXXL\",\n                                     \"4XL\": \"XXXXL\",\n                                     \"5XL\": \"XXXXXL\",\n                                     \"6XL\":\"XXXXXXL\",\n                                     \"7XL\":\"XXXXXXL\"}\n            tmp_dict_size: dict = {}\n            for size in self.size_list:\n                if size in conversion_size.keys():\n                    size = conversion_size[size]\n                tmp_dict_size[size] = order_size[size]\n            ordered_sizes: list = list(\n                {k: v for k, v in sorted(tmp_dict_size.items(), key=lambda item: item[1])}.keys())\n            return ordered_sizes\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- data_processing/data_processing/cleansing_datafeed/size_sorter.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ data_processing/data_processing/cleansing_datafeed/size_sorter.py	(date 1587843111483)
@@ -2,11 +2,20 @@
 
 
 class SizeSorter:
+    """
+    Class to sort the sizes
+    """
+
     def __init__(self, size_list: list):
         self.size_list = size_list
         self.sorting_type: str = self.sorting_type_finder()
+        self.sorted_sizes = self.sort_list()
 
     def sorting_type_finder(self) -> str:
+        """
+        Get the kind of size which are in the given list (e.g. number: 38,40 or letter: XL,S)
+        :return: Type of the sizes
+        """
         letter_counter: int = 0
         number_counter: int = 0
         average_letter: float = 0
@@ -31,26 +40,35 @@
             else:
                 return "number"
 
-    def sort_list(self):
+    def sort_list(self) -> list:
+        """
+        Sort the size list based on its type
+        :return: Sorted size list
+        """
+        ordered_sizes: list = []
         if "OneSize" or "NoSize" in self.size_list:
-            return self.size_list
+            ordered_sizes = self.size_list
         if self.sorting_type == "one_size":
             ordered_sizes = self.size_list
-            return ordered_sizes
+
         if self.sorting_type == "number":
             ordered_sizes = natsort.natsorted(self.size_list)
-            return ordered_sizes
+
         if self.sorting_type == "letter":
             order_size: dict = {"XXXXXS": 0,
                                 "XXXXS": 1,
                                 "XXXS": 2,
                                 "XXS": 3,
                                 "XS": 4,
-                                "S": 5,
-                                "M": 6, "L": 7,
-                                "XL": 8, "XXL": 9, "XXXL": 10, "XXXXL": 11, "XXXXXL": 12,
-                                "XXXXXXL":13,
-                                "XXXXXXXL":14}
+                                "XS-S": 5,
+                                "S": 6,
+                                "S-M": 7,
+                                "M": 8,
+                                "M-L": 9,
+                                "L": 10,
+                                "XL": 11, "XL-XXL": 12, "XXL": 13, "XXXL": 14, "XXXXL": 15, "XXXXXL": 16,
+                                "XXXXXXL": 17,
+                                "XXXXXXXL": 18}
             conversion_size: dict = {"5XS": "XXXXXS",
                                      "4XS": "XXXXS",
                                      "3XS": "XXXS",
@@ -64,13 +82,14 @@
                                      "3XL": "XXXL",
                                      "4XL": "XXXXL",
                                      "5XL": "XXXXXL",
-                                     "6XL":"XXXXXXL",
-                                     "7XL":"XXXXXXL"}
+                                     "6XL": "XXXXXXL",
+                                     "7XL": "XXXXXXL"}
             tmp_dict_size: dict = {}
-            for size in self.size_list:
+            for s, size in enumerate(self.size_list):
                 if size in conversion_size.keys():
                     size = conversion_size[size]
-                tmp_dict_size[size] = order_size[size]
+                if size in order_size.keys():
+                    tmp_dict_size[size] = order_size[size]
             ordered_sizes: list = list(
                 {k: v for k, v in sorted(tmp_dict_size.items(), key=lambda item: item[1])}.keys())
-            return ordered_sizes
+        return ordered_sizes
Index: data_processing/main/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nThis scripts executes the data processing pipe line.\nAt the end of this script the used data feed for the web site will have been\ngenerated.\n\"\"\"\nimport os\nimport sys\n\nfolder = os.path.dirname(os.path.realpath(__file__))\nfolder = folder.replace(\"/data_processing/main\", \"\")\nfolder = folder.replace(r\"\\data_processing\\main\", \"\")\nsys.path.append(folder)\n\nfrom data_processing.data_processing.add_features.add_features import add_features\nfrom data_processing.data_processing.cleansing_datafeed.cleansing_datafeed import cleansing\nfrom data_processing.data_processing.download_data_feeds.download_datafeeds import downloading\nfrom data_processing.data_processing.filter_datafeed.filter_data_feed import filter_data_feed, getArticlesWithLabel, \\\n    delete_non_matching_categories\nfrom data_processing.data_processing.merging_datafeeds.merging_datafeeds_old import merging\n\nimport datetime\n\n\ndef main_app():\n    begin = datetime.datetime.now()\n    print(\"Begin data processing\", begin)\n<<<<<<< HEAD\n    processes: dict = {\"dowloading\": True,\n                       \"merging\": False,\n                       \"filtering\": False,\n                       \"adding_features\": False,\n                       \"filtering_without_label\": False,\n                       \"cleansing\": False,\n                       \"filtering_only_matching_category\": False}\n=======\n    processes: dict = {\"dowloading\": False,\n                       \"merging\": False,\n                       \"filtering\": True,\n                       \"adding_features\": True,\n                       \"filtering_without_label\": True,\n                       \"cleansing\": True,\n                       \"filtering_only_matching_category\": True}\n>>>>>>> 953d6bcfc74b500b9c72a0cd461cedb3f0aa8c8a\n    for process, todo in processes.items():\n        print(process, todo)\n        if process == \"dowloading\":\n            if todo:\n                downloading()\n        if process == \"merging\":\n            if todo:\n                merging()\n        if process == \"filtering\":\n            if todo:\n                filter_data_feed()\n        if process == \"adding_features\":\n            if todo:\n                add_features()\n        if process == \"filtering_without_label\":\n            if todo:\n                getArticlesWithLabel()\n        if process == \"cleansing\":\n            if todo:\n                cleansing()\n        if process == \"filtering_only_matching_category\":\n            if todo:\n                delete_non_matching_categories()\n\n    end = datetime.datetime.now()\n    print(\"End data processing\", end)\n    print(\"It took \", end - begin)\n\n\nmain_app()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- data_processing/main/main.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ data_processing/main/main.py	(date 1587843111499)
@@ -17,30 +17,20 @@
 from data_processing.data_processing.filter_datafeed.filter_data_feed import filter_data_feed, getArticlesWithLabel, \
     delete_non_matching_categories
 from data_processing.data_processing.merging_datafeeds.merging_datafeeds_old import merging
-
 import datetime
 
 
 def main_app():
     begin = datetime.datetime.now()
     print("Begin data processing", begin)
-<<<<<<< HEAD
-    processes: dict = {"dowloading": True,
-                       "merging": False,
-                       "filtering": False,
-                       "adding_features": False,
-                       "filtering_without_label": False,
-                       "cleansing": False,
-                       "filtering_only_matching_category": False}
-=======
+
     processes: dict = {"dowloading": False,
-                       "merging": False,
+                       "merging": True,
                        "filtering": True,
                        "adding_features": True,
                        "filtering_without_label": True,
                        "cleansing": True,
                        "filtering_only_matching_category": True}
->>>>>>> 953d6bcfc74b500b9c72a0cd461cedb3f0aa8c8a
     for process, todo in processes.items():
         print(process, todo)
         if process == "dowloading":
Index: dl_exp/model_architectures/architectures.py
===================================================================
--- dl_exp/model_architectures/architectures.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ dl_exp/model_architectures/architectures.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
@@ -1,53 +0,0 @@
-
-from keras import Model, Input, Sequential, layers
-from keras.layers import Dense, Activation, LSTM, Bidirectional, Embedding, TimeDistributed, Conv1D, \
-    Dropout, MaxPooling1D, concatenate, Flatten
-import keras
-
-
-
-def simple(MAX_LENGTH, NUM_LABELS):
-    """
-    Simple model for classification
-    :param MAX_LENGTH: Maximum length of a sentence
-    :param NUM_LABELS: Number of classes
-    :return: Compiled Model
-    """
-    model = tf.keras.Sequential()
-    model.add(Dense(2048, input_shape=(MAX_LENGTH,)))
-    model.add(Activation('relu'))
-    model.add(Dense(4096, input_shape=(MAX_LENGTH,)))
-    model.add(Activation('relu'))
-    model.add(Dense(8192, input_shape=(MAX_LENGTH,)))
-    model.add(Activation('relu'))
-    model.add(Dense(16384, input_shape=(MAX_LENGTH,)))
-    model.add(Activation('relu'))
-    model.add(Dense(NUM_LABELS))
-    model.add(Activation('softmax'))
-    model.compile(loss='sparse_categorical_crossentropy',
-                  optimizer='adam',
-                  metrics=['accuracy'])
-    return model
-
-def lstm_model(VOCAB_SIZE, EMBEDDING_LENGTH, MAX_LENGTH, NUM_CLASS, COMPILE_MODE, UNITS):
-    model = Sequential()
-    model.add(Embedding(VOCAB_SIZE, EMBEDDING_LENGTH, input_length=MAX_LENGTH))
-    model.add(Dropout(0.5))
-    model.add(Conv1D(filters=UNITS, kernel_size=3, padding='same', activation='relu'))
-    model.add(MaxPooling1D(pool_size=1))
-    model.add(Dropout(0.5))
-    # model.add(Bidirectional(LSTM(UNITS, return_sequences=True)))
-    model.add(Bidirectional(LSTM(UNITS)))
-    model.add(Dense(UNITS * 2, activation='relu'))
-    model.add(Dropout(0.5))
-    if COMPILE_MODE == "categorical":
-        model.add(Dense(NUM_CLASS, activation="softmax"))
-        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
-    if COMPILE_MODE == "binary":
-        model.add(Dense(1, activation="sigmoid"))
-        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
-
-    return model
-
-
-
Index: dl_exp/train/train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import datetime\nimport os, sys\nimport pickle\nfrom random import shuffle\n\nfrom you_conscious.dl_exp.model_architecture.model_architecture import lstm_model\nfrom you_conscious.dl_exp.pre_processing.pre_processing import PreProcessing\nfrom you_conscious.dl_exp.utils.data_utils import DataLoaderCsvTextClassification\n\nPROJECT_FOLDER = os.path.dirname(os.path.abspath(os.getcwd()))\nPROJECT_FOLDER = PROJECT_FOLDER.replace(\"text_classification/main_category_de\", \"\")\nprint(PROJECT_FOLDER)\nsys.path.append(PROJECT_FOLDER)\nprint(PROJECT_FOLDER)\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras_preprocessing.sequence import pad_sequences\n\nimport numpy as np\n\ninput_path: str = \"/home/aurelien/privat_repositories/you_conscious/dl_exp/data/category_training_data_set.csv\"\n\nposition_text: int = 1\nposition_label: int = 0\ninput_path_delimiter: str = \",\"\nclass_delimiter: str = \";\"\nhyper_parameters = {\"max_length\":200,\n                    \"embedding_length\":256,\n                    \"number_epochs\":40,\n                    \"batch_size\":12,\n                    \"number_hidden_lstm_layers\":5,\n                    \"lstm_units\":256\n                    }\ndata_loader = DataLoaderCsvTextClassification(input_path=input_path, position_text=position_text,\n                                              position_label=position_label, input_path_delimiter=input_path_delimiter,\n                                              class_delimiter=class_delimiter)\n\ndata_set = data_loader.load_data_set()\nshuffle(data_set)\nprint(len(data_set))\nprint(data_set[10])\n\n# pre process data\n# pre process string ?\n# put input into int matrices\npre_processor = PreProcessing(data_set)\nword2id, labed2id = pre_processor.create_x2id(data_set)\nprint(\"Vocab size: \", len(word2id))\nprint(labed2id)\ntraining_set = pre_processor.data_set_2_matrices(data_set, pre_processor.word2id, pre_processor.label2id)\n\nprint(training_set[10])\nid2word = {v: k for k, v in word2id.items()}\nid2label = {v: k for k, v in labed2id.items()}\nprint(word2id[\"Wohnung\"])\nprint(id2word[1634])\n\nx_train = []  # list of the text\ny_train = []  # list of the labels\n\nfor data in training_set:\n    text, label = data\n\n    x_train.append(text)\n    y_train.append(label)\nx_train = np.asarray(x_train)\n# add padding\nX_train = pad_sequences(x_train, hyper_parameters[\"max_length\"], dtype='int32', padding='post',\n                        truncating='post'\n                        , value=word2id[\"PADDING_TOKEN\"])\nY_train = np.asarray(y_train)\n\nif len(labed2id) == 2:\n    compileMode = \"binary\"\nelse:\n    compileMode = \"categorical\"\n\n# model\n\nmodel = lstm_model(VOCAB_SIZE=len(word2id), EMBEDDING_LENGTH=hyper_parameters[\"embedding_length\"],\n                   MAX_LENGTH=hyper_parameters[\"max_length\"], NUM_CLASS=len(labed2id),\n                   COMPILE_MODE=compileMode, UNITS=hyper_parameters[\"lstm_units\"],\n                   number_hidden_lstm_layers=hyper_parameters[\"number_hidden_lstm_layers\"])\n\nprint(model.summary())\n# Train the model\nmodel_trained_directory = \"/home/aurelien/privat_repositories/you_conscious/dl_exp/model_trained/test\"\nif not os.path.exists(model_trained_directory):\n    os.mkdir(model_trained_directory)\nmodel_path = os.path.join(model_trained_directory, \"model.h5\")\ncallbacks = [\n    ReduceLROnPlateau(verbose=1),\n    EarlyStopping(patience=100, verbose=1),\n    keras.callbacks.ModelCheckpoint(filepath=model_path, monitor='val_loss', save_best_only=True)\n]\nbegin = datetime.datetime.now().replace(microsecond=0)\n\nhistory_train = model.fit(X_train, [Y_train],\n                          batch_size=hyper_parameters[\"batch_size\"],\n                          epochs=hyper_parameters[\"number_epochs\"],\n                          validation_split=0.3,\n                          class_weight=\"auto\",\n                          callbacks=callbacks,\n                          use_multiprocessing=True\n                          )\nwith open(os.path.join(model_trained_directory, \"word2id.pck\"), \"wb\") as o:\n    pickle.dump(word2id, o)\nwith open(os.path.join(model_trained_directory, \"id2label.pck\"), \"wb\") as o:\n    pickle.dump(id2label, o)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- dl_exp/train/train.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ dl_exp/train/train.py	(date 1587664824501)
@@ -3,15 +3,14 @@
 import pickle
 from random import shuffle
 
-from you_conscious.dl_exp.model_architecture.model_architecture import lstm_model
-from you_conscious.dl_exp.pre_processing.pre_processing import PreProcessing
-from you_conscious.dl_exp.utils.data_utils import DataLoaderCsvTextClassification
+from dl_exp.model_architecture.model_architecture import lstm_model
+from dl_exp.pre_processing.pre_processing import PreProcessing
+from dl_exp.utils.data_utils import DataLoaderCsvTextClassification
 
 PROJECT_FOLDER = os.path.dirname(os.path.abspath(os.getcwd()))
 PROJECT_FOLDER = PROJECT_FOLDER.replace("text_classification/main_category_de", "")
 print(PROJECT_FOLDER)
 sys.path.append(PROJECT_FOLDER)
-print(PROJECT_FOLDER)
 import keras
 from keras.callbacks import ReduceLROnPlateau, EarlyStopping
 from keras_preprocessing.sequence import pad_sequences
@@ -24,12 +23,12 @@
 position_label: int = 0
 input_path_delimiter: str = ","
 class_delimiter: str = ";"
-hyper_parameters = {"max_length":200,
-                    "embedding_length":256,
-                    "number_epochs":40,
-                    "batch_size":12,
-                    "number_hidden_lstm_layers":5,
-                    "lstm_units":256
+hyper_parameters = {"max_length": 200,
+                    "embedding_length": 256,
+                    "number_epochs": 40,
+                    "batch_size": 12,
+                    "number_hidden_lstm_layers": 4,
+                    "lstm_units": 256
                     }
 data_loader = DataLoaderCsvTextClassification(input_path=input_path, position_text=position_text,
                                               position_label=position_label, input_path_delimiter=input_path_delimiter,
@@ -107,3 +106,4 @@
     pickle.dump(word2id, o)
 with open(os.path.join(model_trained_directory, "id2label.pck"), "wb") as o:
     pickle.dump(id2label, o)
+
Index: .idea/dictionaries/graphn.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/dictionaries/graphn.xml	(date 1587842560207)
+++ .idea/dictionaries/graphn.xml	(date 1587842560207)
@@ -0,0 +1,7 @@
+<component name="ProjectDictionaryState">
+  <dictionary name="graphn">
+    <words>
+      <w>sorbas</w>
+    </words>
+  </dictionary>
+</component>
\ No newline at end of file
Index: data_processing/data_processing/cleansing_datafeed/cleansing_datafeed.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from collections import defaultdict\nfrom multiprocessing import Pool\n\nimport natsort\nimport tqdm\n\nfrom data_processing.data_processing.cleansing_datafeed.size import SizeFinder\nfrom data_processing.data_processing.cleansing_datafeed.size_sorter import SizeSorter\nfrom data_processing.data_processing.cleansing_datafeed.utils import clean_category_sex, cleanSize\nfrom data_processing.utils.file_paths import file_paths\nfrom data_processing.utils.getHeaders import getHeadersIndex\nfrom data_processing.utils.utils import createMappingBetween2Columns, files_mapping_categories_path, \\\n    mapping_fashionSuitableFor, synonym_female, synonym_male, synonym_euro, getMappingColumnIndex, \\\n    maxNumberFashionSizeColumns, getLinesCSV, write2File, cleansed_categories_data_feed_path\n\n\nclass Cleanser:\n    def __init__(self):\n        self.input_data_feed: str = file_paths[\"labeled_data_feed_path\"]\n        self.feature_mapping = createMappingBetween2Columns(files_mapping_categories_path, 1, 2, \";\")\n        self.fashionSuitableFor_mapping = createMappingBetween2Columns(mapping_fashionSuitableFor, 2, 6, \";\")\n        self.categoryName_index = getHeadersIndex(\"category_name\")\n        self.fashionSuitableFor_index = getHeadersIndex(\"Fashion:suitable_for\")\n        self.rrp_price_index = getHeadersIndex(\"rrp_price\", file=self.input_data_feed)\n        self.delivery_cost_index = getHeadersIndex(\"delivery_cost\", file=self.input_data_feed)\n        self.search_price_index = getHeadersIndex(\"search_price\", file=self.input_data_feed)\n        self.merchantName_index = getHeadersIndex(\"merchant_name\", file=self.input_data_feed)\n        self.title_index = getHeadersIndex(\"Title\", file=self.input_data_feed)\n\n    def article_cleansing(self, article):\n        \"\"\"\n        First cleansing of the category_name\n        After we cleanse the merchant_name\n        The content in title will also be cleansed. The size, which can be in the title, must be deleted.\n        :param article: Article will be cleansed\n        :return:\n        \"\"\"\n\n        # category_name cleansing\n        content_category_name = article[self.categoryName_index]\n        for string2find, new_category in self.feature_mapping.items():\n            if string2find in content_category_name:\n                article[self.categoryName_index] = new_category\n        article[self.categoryName_index] = clean_category_sex(article)\n        # Change the content within Topman category to man\n        if \"Topman\" in article[self.merchantName_index]:\n            article[self.categoryName_index]: str = article[self.categoryName_index].replace(\"Damen\", \"Herren\")\n\n        # The content in title in stronger than in fashion_suitable:for and fsf in stronger than category_name\n        # Title > fashion_suitable:for > category_name\n        for female_token in synonym_female:\n            if female_token in article[self.title_index]:\n                article[self.categoryName_index]: str = article[self.categoryName_index].replace(\"Herren\", \"Damen\")\n                article[self.fashionSuitableFor_index] = \"Damen\"\n                break\n            if female_token in article[self.fashionSuitableFor_index]:\n                if \"Herren\" == article[self.categoryName_index]:\n                    article[self.categoryName_index]: str = article[self.categoryName_index].replace(\"Herren\", \"Damen\")\n\n        for male_token in synonym_male:\n            if male_token in article[self.title_index]:\n                article[self.categoryName_index]: str = article[self.categoryName_index].replace(\"Damen\", \"Herren\")\n                article[self.fashionSuitableFor_index] = \"Herren\"\n                break\n            if male_token in article[self.fashionSuitableFor_index]:\n                if \"Damen\" == article[self.categoryName_index]:\n                    article[self.categoryName_index]: str = article[self.categoryName_index].replace(\"Damen\", \"Herren\")\n\n        # merchant_name cleansing\n        article[self.merchantName_index] = article[self.merchantName_index].replace(\" DE\", \"\")\n\n        # title cleansing\n        article = self.cleansing_title(article)\n\n        return article\n\n    def cleansing_title(self, article) -> list:\n        \"\"\"\n        Remove the size in the title string\n        :param article:\n        :return: cleansed title\n        \"\"\"\n        title_content: str = article[self.title_index]\n        size_finder:SizeFinder =SizeFinder(str_used=title_content)\n        size_position = size_finder.delete_size()\n        article[self.title_index] = size_position\n        return article\n\n    def cleansing_articles(self, list_vegan_articles):\n        with Pool() as p:\n            result_renamed = list(tqdm.tqdm(p.imap(self.article_cleansing, list_vegan_articles),\n                                            total=len(list_vegan_articles)))\n        return result_renamed\n\n\n    def renamingFashionSuitableFor(self, article):\n        content_categoryName = article[self.categoryName_index]\n        content_fashionSuitableFor = article[self.fashionSuitableFor_index]\n        sex = content_categoryName.split(\" > \")\n        if len(sex) == 0:\n            sex = content_categoryName.split(\">\")\n            sex = sex[1]\n            if content_fashionSuitableFor == \"\" or \" \":\n                article[self.fashionSuitableFor_index] = sex\n        return article\n\n    def renamingFashionSuitableForColumns(self, list_articles):\n        with Pool() as p:\n            result_fashionSuitableFor_renamed = list(tqdm.tqdm(p.imap(self.renamingFashionSuitableFor, list_articles)\n                                                               , total=(len(list_articles))))\n        return result_fashionSuitableFor_renamed\n\n    def cleanPrice(self, article: list) -> list:\n        \"\"\"\n\n        :param article:\n        :return:\n        \"\"\"\n\n        if article[self.rrp_price_index] == \"0\" or article[self.rrp_price_index] == \"0,00\" \\\n                or article[self.rrp_price_index] == \"0.00\":\n            article[self.rrp_price_index] = article[self.search_price_index]\n        if article[self.rrp_price_index] == \"\" or len(article[self.rrp_price_index]) == 0 or article[\n            self.rrp_price_index] == \"0.00 â‚¬\" \\\n                or article[self.rrp_price_index] == \"0.00\":\n            article[self.rrp_price_index] = article[self.search_price_index]\n        if article[self.delivery_cost_index] == '\"0,00 EUR\"' or article[self.delivery_cost_index] == ''\"0.00 EUR\"'':\n            article[self.delivery_cost_index] = \"0\"\n        for euro_token in synonym_euro:\n            if euro_token in article[self.search_price_index]:\n                article[self.search_price_index]: str = article[self.search_price_index].replace(euro_token, \"\")\n            if euro_token in article[self.rrp_price_index]:\n                article[self.rrp_price_index]: str = article[self.rrp_price_index].replace(euro_token, \"\")\n            if euro_token in article[self.delivery_cost_index]:\n                article[self.delivery_cost_index]: str = article[self.delivery_cost_index].replace(euro_token, \"\")\n        article[self.search_price_index]: str = article[self.search_price_index].replace(\".\", \",\")\n        article[self.rrp_price_index]: str = article[self.rrp_price_index].replace(\".\", \",\")\n        article[self.delivery_cost_index]: str = article[self.delivery_cost_index].replace(\".\", \",\")\n        return article\n\n    def cleanPrices(self, list_articles):\n        p = Pool()\n        cleanedPrices_articles = p.map(self.cleanPrice, list_articles)\n        return cleanedPrices_articles\n\n    def mergedProductBySize(self, input_list_articles):\n        \"\"\"\n        Merge product by size, based on \"unique\" the aw_image_url.\n        :param input_list_articles: List of all articles with \"duplicates\" by size\n        :return: list_articles_merged\n        \"\"\"\n        headers = input_list_articles[0]\n        list_art = input_list_articles[1:]\n        list_articles_merged = []\n        mapping_awImageUrl_sizes = defaultdict(list)\n        mapping_awImageUrl_article = {}\n        mapping_columnHeader = getMappingColumnIndex(self.input_data_feed, \"\\t\")\n\n        for article in list_art:\n            size_content = article[mapping_columnHeader[\"Fashion:size\"]]\n            size_content = cleanSize(size_content)\n            mapping_awImageUrl_sizes[article[mapping_columnHeader[\"aw_image_url\"]]].append(\n                size_content)  # Mapping URL sizes\n            mapping_awImageUrl_article[article[mapping_columnHeader[\"aw_image_url\"]]] = article  # Mapping URL article\n        # Add the sizes columns to the headers\n        headers = [header.replace(\"Fashion:size\", \"Fashion:size0\") for header in headers]\n        for i in range(1, maxNumberFashionSizeColumns):  # Start at one because we already use Fashion:size0\n            headers.append(\"Fashion:size\" + str(i))\n\n        mapping_header_columnId = {header: columnId for columnId, header in enumerate(headers)}\n        # Put the size into the size column\n        for url, sizes in mapping_awImageUrl_sizes.items():\n            list_size:list = []\n            for lt in sizes:\n                for size in lt:\n                    list_size.append(size)\n            article = mapping_awImageUrl_article[url]\n            # Append empty rows for the new sizes column\n            for i in range(maxNumberFashionSizeColumns):\n                article.append(\"\")\n            list_size = list_size[:maxNumberFashionSizeColumns]\n            size_sorter:SizeSorter = SizeSorter(list_size)\n            list_size:list = size_sorter.sort_list()\n            for i, size in enumerate(list_size):\n                article[mapping_header_columnId[\"Fashion:size\" + str(i)]] = size\n            list_articles_merged.append(article)\n\n        return [headers] + list_articles_merged\n\n\ndef cleansing():\n    with Pool() as p:\n        clnsr = Cleanser()\n        print(\"Begin cleansing\")\n        list_articles = getLinesCSV(clnsr.input_data_feed, \"\\t\")\n        print(\"Cleansing - Merging by size: Begin\")\n        list_articles = clnsr.mergedProductBySize(list_articles)\n        print(\"Cleansing - Merging by size: Done\")\n        headers = list_articles[0]\n        list_articles = list_articles[1:]\n        print(\"Cleansing - Renaming Categories: Begin\")\n        renamed_category_articles = list(tqdm.tqdm(p.imap(clnsr.article_cleansing, list_articles),\n                                                   total=len(list_articles)))  # clnsr.cleansing_articles(list_articles)\n        renamed_category_articles = [headers] + renamed_category_articles\n        write2File(renamed_category_articles, cleansed_categories_data_feed_path)\n        print(\"Cleansing - Renaming Categories: Done\")\n        headers = renamed_category_articles[0]\n        renamed_category_articles = renamed_category_articles[1:]\n        print(\"Cleansing - Sexes and Prices: Begin\")\n        cleansed_fashion_suitable_for = list(\n            tqdm.tqdm(p.imap(clnsr.renamingFashionSuitableFor, renamed_category_articles)\n                      , total=(len(\n                    renamed_category_articles))))  # clnsr.renamingFashionSuitableForColumns(renamed_category_articles)\n        cleansed_prices = p.map(clnsr.cleanPrice,\n                                cleansed_fashion_suitable_for)  # clnsr.cleanPrices(cleansed_fashion_suitable_for)\n        print(\"Cleansing - Sexes and Prices: Done\")\n        cleansed_articles = [headers] + cleansed_prices\n        write2File(cleansed_articles, file_paths[\"cleansed_sex_data_feed_path\"])\n\n# print(cleanPrice(getLinesCSV(filtered_data_feed_path, \"\\t\")[1]))\n# print(renameCategory(getLinesCSV(filtered_data_feed_path, \"\\t\")[1]))\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- data_processing/data_processing/cleansing_datafeed/cleansing_datafeed.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ data_processing/data_processing/cleansing_datafeed/cleansing_datafeed.py	(date 1587713118170)
@@ -81,7 +81,7 @@
         :return: cleansed title
         """
         title_content: str = article[self.title_index]
-        size_finder:SizeFinder =SizeFinder(str_used=title_content)
+        size_finder: SizeFinder = SizeFinder(str_used=title_content)
         size_position = size_finder.delete_size()
         article[self.title_index] = size_position
         return article
@@ -92,7 +92,6 @@
                                             total=len(list_vegan_articles)))
         return result_renamed
 
-
     def renamingFashionSuitableFor(self, article):
         content_categoryName = article[self.categoryName_index]
         content_fashionSuitableFor = article[self.fashionSuitableFor_index]
@@ -138,10 +137,10 @@
         article[self.delivery_cost_index]: str = article[self.delivery_cost_index].replace(".", ",")
         return article
 
-    def cleanPrices(self, list_articles):
+    def clean_prices(self, list_articles):
         p = Pool()
-        cleanedPrices_articles = p.map(self.cleanPrice, list_articles)
-        return cleanedPrices_articles
+        cleaned_prices_articles = p.map(self.cleanPrice, list_articles)
+        return cleaned_prices_articles
 
     def mergedProductBySize(self, input_list_articles):
         """
@@ -170,7 +169,7 @@
         mapping_header_columnId = {header: columnId for columnId, header in enumerate(headers)}
         # Put the size into the size column
         for url, sizes in mapping_awImageUrl_sizes.items():
-            list_size:list = []
+            list_size: list = []
             for lt in sizes:
                 for size in lt:
                     list_size.append(size)
@@ -179,8 +178,9 @@
             for i in range(maxNumberFashionSizeColumns):
                 article.append("")
             list_size = list_size[:maxNumberFashionSizeColumns]
-            size_sorter:SizeSorter = SizeSorter(list_size)
-            list_size:list = size_sorter.sort_list()
+            print(article)
+            size_sorter: SizeSorter = SizeSorter(list_size)
+            list_size: list = size_sorter.sorted_sizes
             for i, size in enumerate(list_size):
                 article[mapping_header_columnId["Fashion:size" + str(i)]] = size
             list_articles_merged.append(article)
Index: data_processing/data_processing/cleansing_datafeed/test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from data_processing.data_processing.cleansing_datafeed.size_sorter import SizeSorter\n\nl2=[[\"XL\",\"M\",\"S\",\"XS\"],[\"34\",\"32\",\"50\",\"10\"],[\"XS\",\"S\",\"M\",\"L\",\"XL\",\"XXL\",\"XXXL\",\"4XL\",\"XXS\"],[\"S\",\t\"M\",\t\"L\",\"XL\",\t\"XXL\",\t\"XXXL\",\t\"4XL\",\t\"5XL\",\t\"6XL\",\t\"7XL\"]\n]\n\nfor size in l2:\n    size_sorter = SizeSorter(size)\n    print(size_sorter.sort_list())\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- data_processing/data_processing/cleansing_datafeed/test.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ data_processing/data_processing/cleansing_datafeed/test.py	(date 1587665494621)
@@ -1,9 +1,9 @@
 from data_processing.data_processing.cleansing_datafeed.size_sorter import SizeSorter
 
-l2=[["XL","M","S","XS"],["34","32","50","10"],["XS","S","M","L","XL","XXL","XXXL","4XL","XXS"],["S",	"M",	"L","XL",	"XXL",	"XXXL",	"4XL",	"5XL",	"6XL",	"7XL"]
-]
+l2 = [["XL", "M", "S", "XS"], ["34", "32", "50", "10"], ["XS", "S", "M", "L", "XL", "XXL", "XXXL", "4XL", "XXS"],
+      ["S", "M", "L", "XL", "XXL", "XXXL", "4XL", "5XL", "6XL", "7XL"], ["XL-XXL", "S-M", "M-L", "XS-S"],["one_size"]]
 
 for size in l2:
     size_sorter = SizeSorter(size)
-    print(size_sorter.sort_list())
-
+    sl = size_sorter.sort_list()
+    print(size,sl, size_sorter.sorting_type)
Index: .idea/you_conscious.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<module type=\"PYTHON_MODULE\" version=\"4\">\n  <component name=\"NewModuleRootManager\">\n    <content url=\"file://$MODULE_DIR$\">\n      <excludeFolder url=\"file://$MODULE_DIR$/venv\" />\n    </content>\n    <orderEntry type=\"jdk\" jdkName=\"Python 3.8 (you_conscious)\" jdkType=\"Python SDK\" />\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\n  </component>\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/you_conscious.iml	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ .idea/you_conscious.iml	(date 1587840275139)
@@ -4,7 +4,7 @@
     <content url="file://$MODULE_DIR$">
       <excludeFolder url="file://$MODULE_DIR$/venv" />
     </content>
-    <orderEntry type="jdk" jdkName="Python 3.8 (you_conscious)" jdkType="Python SDK" />
+    <orderEntry type="jdk" jdkName="Python 3.8" jdkType="Python SDK" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
 </module>
\ No newline at end of file
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"Python 3.8 (you_conscious)\" project-jdk-type=\"Python SDK\" />\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/misc.xml	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ .idea/misc.xml	(date 1587840275155)
@@ -1,4 +1,4 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8 (you_conscious)" project-jdk-type="Python SDK" />
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8" project-jdk-type="Python SDK" />
 </project>
\ No newline at end of file
Index: data_processing/data_processing/add_features/add_features.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import re\nfrom multiprocessing.pool import Pool\nimport tqdm\n\nfrom data_processing.utils.file_paths import file_paths\nfrom data_processing.utils.getHeaders import getHeadersIndex\nfrom data_processing.utils.utils import getLinesCSV, getMappingColumnIndex, features_mapping_path, affiliateId, \\\n    features_data_feed_path, write2File\n\n\nclass featuresAdder:\n    def __init__(self):\n        self.input_file: str = file_paths[\"filtered_data_feed_path\"]\n        self.features_list = getLinesCSV(features_mapping_path, \";\")[1:]\n        self.mapping_columnHeader = getMappingColumnIndex(self.input_file, \"\\t\")\n        self.awDeepLink_index = getHeadersIndex(\"aw_deep_link\", file=self.input_file)\n\n    def addFeaturesArticle(self, article):\n        \"\"\"\n        Iterate over \"cell\" in the article and search possible features.\n        Add the features in the given column\n        :param article: Article\n        :return: Article\n        \"\"\"\n        for cell in article:\n            for string2Find_feature2Write_columnFeature in self.features_list:\n                string2Find = string2Find_feature2Write_columnFeature[0]\n                feature2Write = string2Find_feature2Write_columnFeature[1]\n                columnFeature = string2Find_feature2Write_columnFeature[2]\n                if re.match(string2Find, cell) is not None:\n                    article[self.mapping_columnHeader[columnFeature]] = feature2Write\n                    break\n                elif string2Find in cell:\n                    article[self.mapping_columnHeader[columnFeature]] = feature2Write\n                    break\n        return article\n\n    def addFeaturesArticles(self, list_articles):\n        with Pool() as p:\n            result_featuredArticles = list(tqdm.tqdm(p.imap(self.addFeaturesArticle, list_articles),\n                                                     total=len(list_articles)))\n        return result_featuredArticles\n\n    def addAffiliateIdArticle(self, article):\n        content_awDeepLink_index = article[self.awDeepLink_index]\n        if \"https://sorbasshoes.com\" in content_awDeepLink_index:\n            for i, char in enumerate(content_awDeepLink_index):\n                if char == \"?\":\n                    link = content_awDeepLink_index[:i] + affiliateId\n                    break\n            article[self.awDeepLink_index] = link\n        return article\n\n    def addAffiliateIdArticles(self, list_articles):\n        with Pool() as p:\n            result_addAffiliateIds = list(tqdm.tqdm(p.imap(self.addAffiliateIdArticle, list_articles),\n                                                    total=len(list_articles)))\n        return result_addAffiliateIds\n\n\ndef add_features():\n    with Pool() as p:\n        ft_adder = featuresAdder()\n        print(\"Begin adding features\")\n        list_articles = getLinesCSV(ft_adder.input_file, \"\\t\")\n        headers = list_articles[0]\n        list_articles = list_articles[1:]\n        print(\"Adding Features - add features: Begin\")\n        list_articles_with_features = list(tqdm.tqdm(p.imap(ft_adder.addFeaturesArticle, list_articles),\n                                                     total=len(list_articles)))#ft_adder.addFeaturesArticles(list_articles)\n        list_articles_with_features = [headers] + list_articles_with_features\n        write2File(list_articles_with_features, features_data_feed_path)\n        print(\"Adding Features - add features: Done\")\n        list_articles = getLinesCSV(features_data_feed_path, \"\\t\")\n        headers = list_articles[0]\n        list_articles = list_articles[1:]\n        print(\"Adding Features - add affiliate ids: Begin\")\n        list_articles_with_affiliateIds = list(tqdm.tqdm(p.imap(ft_adder.addAffiliateIdArticle, list_articles),\n                                                    total=len(list_articles)))#ft_adder.addAffiliateIdArticles(list_articles)\n        print(\"Adding Features - add affiliate ids: Done\")\n        list_articles_with_affiliateIds = [headers] + list_articles_with_affiliateIds\n        write2File(list_articles_with_affiliateIds, file_paths[\"featured_affiliateIds_datafeed_path\"])
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- data_processing/data_processing/add_features/add_features.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ data_processing/data_processing/add_features/add_features.py	(date 1587703142456)
@@ -36,7 +36,7 @@
         return article
 
     def addFeaturesArticles(self, list_articles):
-        with Pool() as p:
+        with Pool(14) as p:
             result_featuredArticles = list(tqdm.tqdm(p.imap(self.addFeaturesArticle, list_articles),
                                                      total=len(list_articles)))
         return result_featuredArticles
@@ -52,14 +52,14 @@
         return article
 
     def addAffiliateIdArticles(self, list_articles):
-        with Pool() as p:
+        with Pool(14) as p:
             result_addAffiliateIds = list(tqdm.tqdm(p.imap(self.addAffiliateIdArticle, list_articles),
                                                     total=len(list_articles)))
         return result_addAffiliateIds
 
 
 def add_features():
-    with Pool() as p:
+    with Pool(14) as p:
         ft_adder = featuresAdder()
         print("Begin adding features")
         list_articles = getLinesCSV(ft_adder.input_file, "\t")
Index: utils/data_dependencies/datafeed-locations_1.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- utils/data_dependencies/datafeed-locations_1.csv	(date 1587530025924)
+++ utils/data_dependencies/datafeed-locations_1.csv	(date 1587530025924)
@@ -0,0 +1,15 @@
+Advertiser Name;URL
+asos;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/23213/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cmerchant_category%2Cisbn%2Cmodel_number%2Cproduct_name%2Cdescription%2Cspecifications%2Cpromotional_text%2Clanguage%2Cmerchant_deep_link%2Cmerchant_image_url%2Cdelivery_time%2Ccurrency%2Csearch_price%2Crrp_price%2Cdelivery_cost%2Cin_stock%2Cstock_quantity%2Cproduct_type%2Ccolour%2Ccustom_1%2Ccustom_2%2CFashion%3Asuitable_for%2CFashion%3Asize%2Crating%2Calternate_image%2Cmerchant_product_category_path/
+Vero Moda;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/20233/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cmerchant_category%2Cean%2Cmpn%2Cisbn%2Cmodel_number%2Cproduct_name%2Cdescription%2Cspecifications%2Clanguage%2Cmerchant_deep_link%2Cmerchant_thumb_url%2Cmerchant_image_url%2Cdelivery_time%2Cvalid_from%2Cvalid_to%2Ccurrency%2Csearch_price%2Cstore_price%2Cdelivery_cost%2Cweb_offer%2Cpre_order%2Cin_stock%2Cstock_quantity%2Cwarranty%2Ccondition%2Cparent_product_id%2Ccommission_group%2Clast_updated%2Cdimensions%2Ccolour%2Ckeywords%2Ccustom_1%2Ccustom_2%2Csaving%2CFashion%3Asuitable_for%2CFashion%3Asize%2CFashion%3Amaterial%2CFashion%3Apattern%2CFashion%3Aswatch%2Crating%2Calternate_image%2Clarge_image%2Cbasket_link%2Cproduct_short_description%2Cmerchant_product_category_path%2Cmerchant_product_second_category%2Cmerchant_product_third_category%2Csavings_percent%2Cproduct_price_old%2Calternate_image_two%2Calternate_image_three%2Calternate_image_four/
+Hessnatur;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/32493/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cean%2Cproduct_name%2Cdescription%2Cspecifications%2Cmerchant_deep_link%2Cmerchant_image_url%2Cdelivery_time%2Ccurrency%2Csearch_price%2Cdelivery_cost%2Ccolour%2Ckeywords%2CFashion%3Asuitable_for%2CFashion%3Asize%2Cmerchant_product_category_path%2Cproduct_price_old%2Cstock_status/
+imwalking;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/35647/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cmerchant_category%2Cean%2Cproduct_name%2Cdescription%2Cmerchant_deep_link%2Cmerchant_image_url%2Cdelivery_time%2Ccurrency%2Csearch_price%2Cdelivery_cost%2Cin_stock%2Ccolour%2CFashion%3Asuitable_for%2CFashion%3Asize%2Cmerchant_product_second_category%2Cproduct_price_old/
+Hallhuber;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/34489/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cean%2Cisbn%2Cproduct_name%2Cdescription%2Cmerchant_deep_link%2Cmerchant_image_url%2Cdelivery_time%2Cvalid_from%2Cvalid_to%2Ccurrency%2Csearch_price%2Cdelivery_cost%2Ccolour%2Ckeywords%2CFashion%3Asuitable_for%2CFashion%3Asize%2Cproduct_short_description%2Cmerchant_product_category_path/
+ABOUT YOU;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/33943/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cean%2Cproduct_name%2Cdescription%2Cmerchant_deep_link%2Cmerchant_image_url%2Ccurrency%2Csearch_price%2Cdelivery_cost%2Ccolour%2CFashion%3Asuitable_for%2CFashion%3Asize%2CFashion%3Amaterial%2Calternate_image%2Clarge_image%2Cmerchant_product_category_path%2Cproduct_price_old%2Cstock_status%2Calternate_image_two%2Calternate_image_three%2Calternate_image_four/
+Guess;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/22045/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cmerchant_category%2Cean%2Cproduct_name%2Cdescription%2Cmerchant_deep_link%2Cdelivery_time%2Ccurrency%2Csearch_price%2Ccolour%2Ccustom_1%2Ccustom_2%2CFashion%3Asize%2Clarge_image%2Cproduct_price_old%2Cstock_status/
+Humanic;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/30531/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cean%2Cproduct_name%2Cdescription%2Cmerchant_deep_link%2Cvalid_from%2Cvalid_to%2Ccurrency%2Csearch_price%2Cdelivery_cost%2Ccolour%2Ckeywords%2Ccustom_1%2Ccustom_2%2Ccustom_3%2Ccustom_4%2Ccustom_5%2Cdelivery_restrictions%2CFashion%3Asize%2Clarge_image%2Cproduct_short_description%2Cmerchant_product_category_path%2Cbase_price%2Cbase_price_amount%2Cbase_price_text%2Cproduct_price_old%2Cterms_of_contract%2Ccustom_6%2Ccustom_7%2Ccustom_8/
+promod;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/27329/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cmerchant_category%2Cproduct_name%2Cdescription%2Cmerchant_deep_link%2Cmerchant_thumb_url%2Cmerchant_image_url%2Csearch_price%2Cdelivery_cost%2Ccolour%2Cdelivery_restrictions%2CFashion%3Asuitable_for%2CFashion%3Asize%2CFashion%3Amaterial%2Calternate_image%2Clarge_image%2Cproduct_price_old%2Calternate_image_two%2Calternate_image_three/
+mirapodo;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/31135/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cmerchant_category%2Cean%2Cproduct_name%2Cdescription%2Cmerchant_deep_link%2Cmerchant_image_url%2Cdelivery_time%2Cvalid_from%2Cvalid_to%2Ccurrency%2Csearch_price%2Cdelivery_cost%2Cstock_quantity%2Cparent_product_id%2Ccolour%2Ckeywords%2Ccustom_1%2Ccustom_2%2Ccustom_3%2Ccustom_4%2Ccustom_5%2Cdelivery_restrictions%2CFashion%3Asuitable_for%2CFashion%3Asize%2Cproduct_short_description%2Cbase_price%2Cbase_price_amount%2Cbase_price_text%2Cproduct_price_old%2Cstock_status%2Ccustom_6%2Ccustom_7%2Ccustom_8/
+SORBAS;https://sorbasshoes.com/wp-content/uploads/woo-product-feed-pro/csv/95d085133d0a2f685dd9c28ec48b850c.csv
+OTTO;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/34239/format/csv/language/de/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cmerchant_category%2Cean%2Cmpn%2Cproduct_name%2Cdescription%2Cspecifications%2Cmerchant_deep_link%2Cmerchant_image_url%2Cdelivery_time%2Ccurrency%2Csearch_price%2Cdelivery_cost%2Ccolour%2Ccustom_1%2Ccustom_2%2CFashion%3Asuitable_for%2CFashion%3Asize%2CFashion%3Amaterial%2Calternate_image%2Cmerchant_product_category_path%2Cmerchant_product_second_category%2Cmerchant_product_third_category%2Cbase_price%2Cbase_price_text%2Cproduct_price_old%2Calternate_image_two%2Calternate_image_three%2Calternate_image_four/
+Topman;https://productdata.awin.com/datafeed/download/apikey/74184eddf378e7ff21c51b3e42ff4de6/fid/26211/format/csv/language/en/delimiter/%2C/compression/gzip/columns/data_feed_id%2Cmerchant_id%2Cmerchant_name%2Caw_product_id%2Caw_deep_link%2Caw_image_url%2Caw_thumb_url%2Ccategory_id%2Ccategory_name%2Cbrand_id%2Cbrand_name%2Cmerchant_product_id%2Cmerchant_category%2Cisbn%2Cmodel_number%2Cproduct_name%2Cdescription%2Cspecifications%2Cpromotional_text%2Clanguage%2Cmerchant_deep_link%2Cmerchant_image_url%2Cdelivery_time%2Ccurrency%2Csearch_price%2Crrp_price%2Cdelivery_cost%2Cin_stock%2Cstock_quantity%2Ccolour%2Ccustom_1%2CFashion%3Asuitable_for%2CFashion%3Asize%2CFashion%3Amaterial%2Calternate_image%2Cproduct_short_description%2Calternate_image_two/
+LOVECO;https://www.webgains.com/affiliates/datafeed.html?action=download&campaign=1332125&feeds=16895&categories=1555,9454,1559,1560,1563,21633,21637,1556,1574,1577,1578,1582,1583,1590,1620,1637,16644,16645,1644,1649,1650,1651,1652,1653,1654,1655,1661,1662,1663,21613,21615,1668,1669,1670,1671,1673,1707,1708,1709,1685,1686,20119,20123,1684,12827,16995,20101,20102,1692,1693,1734,1735,19692,20183,20184,20173,20175,1777,8722,20108,8730,19320,8729,8721,19969,8725,1938&fields=extended&fieldIds=description,image_url,deeplink,category_id,merchant_category,category_name,category_path,price,product_id,product_name,program_id,program_name,last_updated,in_stock,payment_methods,image_large_url,european_article_number,Colour,gender,size,image_url,keywords,short_description,delivery_period,brand,Fabric,merchant_category_id,normal_price,seals,delivery_cost,ship_to,currency&format=csv&separator=comma&zipformat=none&stripNewlines=0&apikey=6b98009433d110ce214bb399e6f414e0
Index: data_processing/data_processing/filter_datafeed/filter_data_feed.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from multiprocessing import Pool\nfrom typing import Union\n\nfrom nltk import word_tokenize\nimport tqdm\n\nfrom data_processing.data_processing.filter_datafeed.utils import getFilters\nfrom data_processing.utils.file_paths import file_paths\nfrom data_processing.utils.getHeaders import getHeadersIndex\nfrom data_processing.utils.utils import filters_file_path, getLinesCSV, merged_data_feed_path, \\\n    write2File\n\n\nclass Filter:\n\n    def __init__(self):\n        self.vegan_filters = getFilters(filters_file_path)\n        self.vegan_filters.sort()\n        try:\n            self.label_index_feature_datafeed = getHeadersIndex(\"Labels\",\n                                                                file_paths[\"featured_affiliateIds_datafeed_path\"])\n            self.material_index_feature_datafeed = getHeadersIndex(\"Material\",\n                                                                   file_paths[\"featured_affiliateIds_datafeed_path\"])\n            self.category_name_index = getHeadersIndex(\"category_name\", file_paths[\"cleansed_sex_data_feed_path\"])\n        except:\n            pass\n\n    def isArticleVegan(self, article: list) -> Union[None, list]:\n        \"\"\"\n        :param article: Article's line in a list format\n        :param vegan_filters: List of filter to be apply in oder to know if the article is vegan or not\n        :return: The article's line if it is vegan\n        \"\"\"\n        vegan: bool = bool\n        tmp_article = article\n        article = \" \".join(article)\n        article_words = sorted(set(word_tokenize(article.replace(\",\", \" \"))))\n        article_words = list(article_words)\n        vegan = True\n        for filter_veg in self.vegan_filters:\n            filter_veg = filter_veg.split(\" \")\n            if len(set(filter_veg).intersection(article_words)) == len(filter_veg):\n                vegan = False\n                break\n        if vegan:\n            return tmp_article\n\n    def getListVeganArticles(self, list_articles):\n        \"\"\"\n        Iteration over all article in the list and create list of vegan article filtered with the filters\n        :param list_articles: list of all articles\n        :param vegan_filters: filtered vegan articles\n        \"\"\"\n        with Pool() as p:\n            result_vegan = list(tqdm.tqdm(p.imap(self.isArticleVegan, list_articles), total=len(list_articles)))\n        return result_vegan\n\n    def removeArticleWithNoLabel(self, article: list) -> list:\n        \"\"\"\n        :param article: Article in a list format\n        :return: Return the article if it has a label\n        \"\"\"\n\n        if article[self.label_index_feature_datafeed] or article[self.material_index_feature_datafeed] != \"\":\n            return article\n\n    def removeArticlesWithNoLabel(self, list_articles: list) -> list:\n        with Pool() as p:\n            list_articles_with_label = list(tqdm.tqdm(p.imap(self.removeArticleWithNoLabel, list_articles),\n                                                      total=len(list_articles)))\n\n        return list_articles_with_label\n\n    def delete_non_matching_category(self, article):\n        \"\"\"\n        Does not return an article if Damen or Herren is not in the category_name\n        :param article: Article to be processed\n        :return: Article if Herren or Damen is in the category_name\n        \"\"\"\n        category_name_content: str = article[self.category_name_index]\n        category_name_content_tokens: list = word_tokenize(category_name_content)\n        to_return:bool = False\n\n        if \"Herren\" in category_name_content_tokens:\n            to_return = True\n        if \"Damen\" in category_name_content_tokens:\n            to_return = True\n        if to_return:\n            return article\n\n    def delete_non_matching_categories(self, list_articles: list) -> list:\n        with Pool() as p:\n            deleted_non_matching_categories: list = list(\n                tqdm.tqdm(p.imap(self.delete_non_matching_category, list_articles),\n                          total=len(list_articles)))\n\n        return deleted_non_matching_categories\n\n\ndef filter_data_feed():\n    fltr = Filter()\n    print(\"Filtering script has begun \")\n    print(\"List filters has been created.\")\n    list_articles = getLinesCSV(merged_data_feed_path, \",\")\n    headers = list_articles[0]\n    list_articles = list_articles[1:]\n    print(\"List of articles has been created\")\n    print(\"Filtering - filtering: Begin\")\n    list_articles_vegan = fltr.getListVeganArticles(list_articles)\n    print(\"Filtering - filtering: Done\")\n    list_articles_vegan = [headers] + list_articles_vegan\n    write2File(list_articles_vegan, file_paths[\"filtered_data_feed_path\"])\n    print(\"Filtering: Done\")\n\n\ndef getArticlesWithLabel():\n    fltr = Filter()\n    list_articles = getLinesCSV(file_paths[\"featured_affiliateIds_datafeed_path\"], \"\\t\")\n    headers = list_articles[0]\n    list_articles = list_articles[1:]\n    print(\"Filtering - remove articles without label: Begin\")\n    list_articles_withLabel = fltr.removeArticlesWithNoLabel(list_articles)\n    print(\"Filtering - remove articles without label: End\")\n    list_articles_withLabel = [headers] + list_articles_withLabel\n    write2File(list_articles_withLabel, file_paths[\"labeled_data_feed_path\"])\n    print(\"Removed all articles without label\")\n\n\ndef delete_non_matching_categories():\n    flt = Filter()\n    print(\"Delete non matching categories : Begin\")\n    list_articles = getLinesCSV(file_paths[\"cleansed_sex_data_feed_path\"],\"\\t\")\n    headers = list_articles[0]\n    list_articles = list_articles[1:]\n    deleted_non_matching_categories_articles = flt.delete_non_matching_categories(\n        list_articles)\n    list_articles = [headers] + deleted_non_matching_categories_articles\n    write2File(list_articles,file_paths[\"filtered_only_matching_categories_datafeed\"])\n    print(\"Delete non matching categories : Done\")\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- data_processing/data_processing/filter_datafeed/filter_data_feed.py	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ data_processing/data_processing/filter_datafeed/filter_data_feed.py	(date 1587703142464)
@@ -28,10 +28,8 @@
     def isArticleVegan(self, article: list) -> Union[None, list]:
         """
         :param article: Article's line in a list format
-        :param vegan_filters: List of filter to be apply in oder to know if the article is vegan or not
         :return: The article's line if it is vegan
         """
-        vegan: bool = bool
         tmp_article = article
         article = " ".join(article)
         article_words = sorted(set(word_tokenize(article.replace(",", " "))))
@@ -49,7 +47,6 @@
         """
         Iteration over all article in the list and create list of vegan article filtered with the filters
         :param list_articles: list of all articles
-        :param vegan_filters: filtered vegan articles
         """
         with Pool() as p:
             result_vegan = list(tqdm.tqdm(p.imap(self.isArticleVegan, list_articles), total=len(list_articles)))
@@ -64,14 +61,14 @@
         if article[self.label_index_feature_datafeed] or article[self.material_index_feature_datafeed] != "":
             return article
 
-    def removeArticlesWithNoLabel(self, list_articles: list) -> list:
-        with Pool() as p:
+    def remove_articles_with_no_label(self, list_articles: list) -> list:
+        with Pool(14) as p:
             list_articles_with_label = list(tqdm.tqdm(p.imap(self.removeArticleWithNoLabel, list_articles),
                                                       total=len(list_articles)))
 
         return list_articles_with_label
 
-    def delete_non_matching_category(self, article):
+    def delete_non_matching_category(self, article) -> list:
         """
         Does not return an article if Damen or Herren is not in the category_name
         :param article: Article to be processed
@@ -79,7 +76,7 @@
         """
         category_name_content: str = article[self.category_name_index]
         category_name_content_tokens: list = word_tokenize(category_name_content)
-        to_return:bool = False
+        to_return: bool = False
 
         if "Herren" in category_name_content_tokens:
             to_return = True
@@ -119,7 +116,7 @@
     headers = list_articles[0]
     list_articles = list_articles[1:]
     print("Filtering - remove articles without label: Begin")
-    list_articles_withLabel = fltr.removeArticlesWithNoLabel(list_articles)
+    list_articles_withLabel = fltr.remove_articles_with_no_label(list_articles)
     print("Filtering - remove articles without label: End")
     list_articles_withLabel = [headers] + list_articles_withLabel
     write2File(list_articles_withLabel, file_paths["labeled_data_feed_path"])
@@ -129,11 +126,11 @@
 def delete_non_matching_categories():
     flt = Filter()
     print("Delete non matching categories : Begin")
-    list_articles = getLinesCSV(file_paths["cleansed_sex_data_feed_path"],"\t")
+    list_articles = getLinesCSV(file_paths["cleansed_sex_data_feed_path"], "\t")
     headers = list_articles[0]
     list_articles = list_articles[1:]
     deleted_non_matching_categories_articles = flt.delete_non_matching_categories(
         list_articles)
     list_articles = [headers] + deleted_non_matching_categories_articles
-    write2File(list_articles,file_paths["filtered_only_matching_categories_datafeed"])
+    write2File(list_articles, file_paths["filtered_only_matching_categories_datafeed"])
     print("Delete non matching categories : Done")
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>absl-py==0.9.0\nastor==0.8.1\ncachetools==4.1.0\ncertifi==2020.4.5.1\nchardet==3.0.4\ngast==0.2.2\ngoogle-auth==1.14.1\ngoogle-auth-oauthlib==0.4.1\ngoogle-pasta==0.2.0\ngrpcio==1.28.1\nh5py==2.10.0\nidna==2.9\nKeras==2.3.1\nKeras-Applications==1.0.8\nKeras-Preprocessing==1.1.0\nMarkdown==3.2.1\nnumpy==1.18.3\noauthlib==3.1.0\nopt-einsum==3.2.1\nprotobuf==3.11.3\npyasn1==0.4.8\npyasn1-modules==0.2.8\nPyYAML==5.3.1\nrequests==2.23.0\nrequests-oauthlib==1.3.0\nrsa==4.0\nscipy==1.4.1\nsix==1.14.0\ntensorboard==2.1.1\ntensorflow==2.1.0\ntensorflow-estimator==2.1.0\ntermcolor==1.1.0\nurllib3==1.25.9\nWerkzeug==1.0.1\nwrapt==1.12.1\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- requirements.txt	(revision 457383f6b52532a3e764916f3c789169bc9e9696)
+++ requirements.txt	(date 1587661848809)
@@ -3,6 +3,7 @@
 cachetools==4.1.0
 certifi==2020.4.5.1
 chardet==3.0.4
+click==7.1.1
 gast==0.2.2
 google-auth==1.14.1
 google-auth-oauthlib==0.4.1
@@ -10,17 +11,22 @@
 grpcio==1.28.1
 h5py==2.10.0
 idna==2.9
+joblib==0.14.1
 Keras==2.3.1
 Keras-Applications==1.0.8
 Keras-Preprocessing==1.1.0
 Markdown==3.2.1
+natsort==7.0.1
+nltk==3.5
 numpy==1.18.3
 oauthlib==3.1.0
 opt-einsum==3.2.1
+progressbar==2.5
 protobuf==3.11.3
 pyasn1==0.4.8
 pyasn1-modules==0.2.8
 PyYAML==5.3.1
+regex==2020.4.4
 requests==2.23.0
 requests-oauthlib==1.3.0
 rsa==4.0
@@ -30,6 +36,7 @@
 tensorflow==2.1.0
 tensorflow-estimator==2.1.0
 termcolor==1.1.0
+tqdm==4.45.0
 urllib3==1.25.9
 Werkzeug==1.0.1
 wrapt==1.12.1
diff --git dl_exp/model_architectures/__init__.py dl_exp/model_architectures/__init__.py
deleted file mode 100644
