{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9c21dbf9e647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data_processing/data_processing/cleansing_datafeed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+++++\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "folder = os.path.dirname(os.path.realpath(__file__))\n",
    "folder = folder.replace(\"/data_processing/data_processing/cleansing_datafeed\", \"\")\n",
    "print(\"+++++\",folder)\n",
    "sys.path.append(folder)\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import tqdm\n",
    "#from farm.infer import Inferencer\n",
    "\n",
    "from data_processing.data_processing.cleansing_datafeed.config import merchant_to_identifier\n",
    "from data_processing.data_processing.cleansing_datafeed.size_finder import SizeFinder\n",
    "from data_processing.data_processing.cleansing_datafeed.size_sorter import SizeSorter\n",
    "from data_processing.data_processing.cleansing_datafeed.utils import clean_category_sex, clean_size\n",
    "from data_processing.data_processing.filter_datafeed.utils import replace_merchant_names_ean_order\n",
    "from data_processing.data_processing.utils.columns_order import column_index_mapping\n",
    "from data_processing.data_processing.utils.file_paths import file_paths\n",
    "from data_processing.data_processing.utils.getHeaders import get_headers_index\n",
    "from data_processing.data_processing.utils.merchant_ean_ranking import ranking_merchant_ean\n",
    "from data_processing.data_processing.utils.utils import create_mapping_between_2_columns, \\\n",
    "    files_mapping_categories_path, \\\n",
    "    mapping_fashionSuitableFor, synonym_female, synonym_male, synonym_euro, get_mapping_column_index, \\\n",
    "    maxNumberFashionSizeColumns, get_lines_csv, write_2_file, get_tokens\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "class Cleanser:\n",
    "    def __init__(self):\n",
    "        self.input_data_feed: str = file_paths[\"labeled_data_feed_path\"]\n",
    "        self.category_name_cleansing: str = file_paths[\"category_name_cleansing\"]\n",
    "        self.category_name_cleansing_conditions: list = get_lines_csv(self.category_name_cleansing, \",\")[1:]\n",
    "        self.data = pd.read_csv(self.input_data_feed, low_memory=False, sep=\"\\t\")\n",
    "        self.columns = list(self.data.columns)\n",
    "        self.column_2_id = {column: i for i, column in enumerate(self.columns)}\n",
    "        self.feature_mapping = create_mapping_between_2_columns(files_mapping_categories_path, 1, 2, \",\")\n",
    "        self.fashionSuitableFor_mapping = create_mapping_between_2_columns(mapping_fashionSuitableFor, 2, 6, \";\")\n",
    "        self.category_name_index = get_headers_index(\"category_name\")\n",
    "        self.description_index = get_headers_index(\"description\")\n",
    "        self.fashionSuitableFor_index = get_headers_index(\"Fashion:suitable_for\")\n",
    "        self.rrp_price_index = get_headers_index(\"rrp_price\", file=self.input_data_feed)\n",
    "        self.keyword_index = get_headers_index(\"keyword\", file=self.input_data_feed)\n",
    "\n",
    "        self.delivery_cost_index = get_headers_index(\"delivery_cost\", file=self.input_data_feed)\n",
    "        self.search_price_index = get_headers_index(\"search_price\", file=self.input_data_feed)\n",
    "        self.merchantName_index = self.column_2_id[\n",
    "            \"merchant_name\"]  # get_headers_index(\"merchant_name\", file=self.input_data_feed)\n",
    "        self.title_index = get_headers_index(\"Title\", file=self.input_data_feed)\n",
    "        self.merchant_product_id_index = get_headers_index(\"merchant_product_id\", file=self.input_data_feed)\n",
    "        self.colour_index = get_headers_index(\"colour\", file=self.input_data_feed)\n",
    "        self.aw_deep_link_index = get_headers_index(\"aw_deep_link\", file=self.input_data_feed)\n",
    "        self.item_id_index = get_headers_index(\"item_id\", file=self.input_data_feed)\n",
    "        self.model_path_categories = \"/home/graphn/repositories/you_conscious/dl_xp/trained_models/category_experiment_2021_04_03\"\n",
    "        self.model_path_colors = \"/home/graphn/repositories/you_conscious/dl_xp/trained_models/color\"\n",
    "        self.model_path_saison = \"/home/graphn/repositories/you_conscious/dl_xp/trained_models/saison\"\n",
    "        self.model_path_origin = \"/home/graphn/repositories/you_conscious/dl_xp/trained_models/origin\"\n",
    "        self.model_path_keywords = \"/home/graphn/repositories/you_conscious/dl_xp/trained_models/keywords\"\n",
    "        self.column_features = [\"brand\",\n",
    "                                \"merchant_name\",\n",
    "                                \"Fashion:suitable_for\",\n",
    "                                \"Title\",\n",
    "                                \"description\"]\n",
    "        self.column_features_keywords = [\"Title\"]\n",
    "        self.column_features_origin = [\"Title\", \"description\"]\n",
    "        self.column_id_mapping = column_index_mapping\n",
    "        self.unwanted_replacement_string = {\"<div>\": \"\", \"<ul>\": \"\", \"<li>\": \"|\", \"<span>\": \"\", \"</span>\": \"\",\n",
    "                                            \"<br>\": \"|\", \"</li>\": \"\", \"</ul>\": \"\", \"</div>\": \"\",\n",
    "                                            \"&lt;/div&gt;\": \"\", \"&lt;div&gt;\": \"\", \"&lt;ul&gt;\": \"\", \"&lt;li&gt;\": \"\",\n",
    "                                            \"&lt;span&gt;\": \"\", \"&lt;/span&gt;\": \"\", \"&lt;br&gt;\": \"\",\n",
    "                                            \"&lt;/li&gt;\": \"\", \"&lt;/ul&gt;\": \"\"\n",
    "                                            }\n",
    "\n",
    "    def article_cleansing(self, article: list) -> list:\n",
    "        \"\"\"\n",
    "        Cleansing of the category_name, merchant_name, fashion suitable for\n",
    "        The content in title will also be cleansed. The size, which can be in the title, must be deleted.\n",
    "        :param article: Article will be cleansed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # category_name cleansing\n",
    "        content_category_name = article[self.category_name_index]\n",
    "        content_category_tokens: list = get_tokens(content_category_name)\n",
    "        for string2find, new_category in self.feature_mapping.items():\n",
    "            if string2find in content_category_name:\n",
    "                article[self.category_name_index] = new_category\n",
    "        article[self.category_name_index] = clean_category_sex(article)\n",
    "\n",
    "        # Change the content within Topman category to man\n",
    "        if \"Topman\" in article[self.merchantName_index] or \"Uli Schott\":\n",
    "            article[self.category_name_index]: str = article[self.category_name_index].replace(\"Damen\", \"Herren\")\n",
    "\n",
    "        # The content in title is stronger than in fashion_suitable:for and fsf in stronger than category_name\n",
    "        # Title > fashion_suitable:for > category_name\n",
    "        title_content: str = article[self.title_index]\n",
    "        title_tokens: list = get_tokens(title_content)\n",
    "        fashion_suitable_for_content = article[self.fashionSuitableFor_index]\n",
    "        fashion_suitable_for_tokens: list = get_tokens(fashion_suitable_for_content)\n",
    "\n",
    "        for female_token in synonym_female:\n",
    "            if female_token in title_tokens:\n",
    "                article[self.category_name_index]: str = article[self.category_name_index].replace(\"Herren\", \"Damen\")\n",
    "                article[self.fashionSuitableFor_index] = \"Damen\"\n",
    "                break\n",
    "            if female_token in fashion_suitable_for_tokens:\n",
    "                if \"Herren\" in article[self.category_name_index]:\n",
    "                    article[self.category_name_index]: str = article[self.category_name_index].replace(\"Herren\",\n",
    "                                                                                                       \"Damen\")\n",
    "\n",
    "        for male_token in synonym_male:\n",
    "            if male_token in title_tokens:\n",
    "                article[self.category_name_index]: str = article[self.category_name_index].replace(\"Damen\", \"Herren\")\n",
    "                article[self.fashionSuitableFor_index] = \"Herren\"\n",
    "                break\n",
    "            if male_token in fashion_suitable_for_tokens:\n",
    "                if \"Damen\" in article[self.category_name_index]:\n",
    "                    article[self.category_name_index]: str = article[self.category_name_index].replace(\"Damen\",\n",
    "                                                                                                       \"Herren\")\n",
    "\n",
    "        # Clean fashion_suitable_:for\n",
    "        if \"Female\" == article[self.fashionSuitableFor_index]:\n",
    "            article[self.fashionSuitableFor_index] = \"Damen\"\n",
    "        if \"Male\" == article[self.fashionSuitableFor_index]:\n",
    "            article[self.fashionSuitableFor_index] = \"Herren\"\n",
    "        # merchant_name cleansing\n",
    "        article[self.merchantName_index] = article[self.merchantName_index].replace(\" DE\", \"\")\n",
    "\n",
    "        # title cleansing\n",
    "        article = self.cleansing_title(article)\n",
    "\n",
    "        # category name cleansing\n",
    "        article = self.cleansing_category_names(article, content_category_tokens)\n",
    "\n",
    "        # description cleansing\n",
    "        article[self.description_index] = self.cleansing_description(article[self.description_index])\n",
    "\n",
    "        # in_stock cleansing\n",
    "        if article[self.column_id_mapping[\"in_stock\"]] == \"1\":\n",
    "            article[self.column_id_mapping[\"in_stock\"]] = \"Ja\"\n",
    "        if article[self.column_id_mapping[\"stock_status\"]] == \"JA\" or article[self.column_id_mapping\n",
    "        [\"stock_status\"]] == \"in stock\":\n",
    "            article[self.column_id_mapping[\"stock_status\"]] = \"Verfügbar\"\n",
    "        return article\n",
    "\n",
    "    def cleansing_description(self, description: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleanse a description\n",
    "        :param description:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for string_to_find, replacement in self.unwanted_replacement_string.items():\n",
    "            description = description.replace(string_to_find, replacement)\n",
    "        return description\n",
    "\n",
    "    def cleansing_articles(self, list_vegan_articles):\n",
    "        with Pool(16) as p:\n",
    "            result_renamed = list(tqdm.tqdm(p.imap(self.article_cleansing, list_vegan_articles),\n",
    "                                            total=len(list_vegan_articles)))\n",
    "        return result_renamed\n",
    "\n",
    "    def cleansing_title(self, article) -> list:\n",
    "        \"\"\"\n",
    "        Remove the size in the title string\n",
    "        :param article:\n",
    "        :return: cleansed title\n",
    "        \"\"\"\n",
    "        title_content: str = article[self.title_index]\n",
    "        size_finder: SizeFinder = SizeFinder(str_used=title_content)\n",
    "        size_position = size_finder.delete_size()\n",
    "        article[self.title_index] = size_position\n",
    "        return article\n",
    "\n",
    "    def cleansing_category_names(self, article: list, content_category_tokens: list) -> list:\n",
    "        for condition in self.category_name_cleansing_conditions:\n",
    "            frm = condition[0]\n",
    "            from_category_name_condition = condition[1]\n",
    "            to = condition[2]\n",
    "            if frm in content_category_tokens and from_category_name_condition in content_category_tokens:\n",
    "                content_category_content: str = \" \".join(content_category_tokens)\n",
    "                content_category_content = content_category_content.replace(frm, \"\")\n",
    "                content_category_content = content_category_content.replace(\"&\", \"\")\n",
    "                article[self.category_name_index] = content_category_content.replace(frm, to)\n",
    "                break\n",
    "        return article\n",
    "\n",
    "    def renaming_fashion_suitable_for(self, article) -> list:\n",
    "        content_category_name = article[self.category_name_index]\n",
    "        content_fashion_suitable_for = article[self.fashionSuitableFor_index]\n",
    "        sex = content_category_name.split(\" > \")\n",
    "        if len(sex) == 0:\n",
    "            sex = content_category_name.split(\">\")\n",
    "            sex = sex[1]\n",
    "            if content_fashion_suitable_for == \"\" or \" \":\n",
    "                article[self.fashionSuitableFor_index] = sex\n",
    "        return article\n",
    "\n",
    "    def renaming_fashion_suitable_for_columns(self, list_articles):\n",
    "        with Pool() as p:\n",
    "            result_fashion_suitable_for_renamed = list(tqdm.tqdm(p.imap(self.renaming_fashion_suitable_for,\n",
    "                                                                        list_articles),\n",
    "                                                                 total=(len(list_articles))))\n",
    "        return result_fashion_suitable_for_renamed\n",
    "\n",
    "    def clean_price(self, article: list) -> list:\n",
    "        \"\"\"\n",
    "\n",
    "        :param article:\n",
    "        :return: article\n",
    "        \"\"\"\n",
    "\n",
    "        if article[self.rrp_price_index] == \"0\" or article[self.rrp_price_index] == \"0,00\" \\\n",
    "                or article[self.rrp_price_index] == \"0.00\":\n",
    "            article[self.rrp_price_index] = article[self.search_price_index]\n",
    "        if article[self.rrp_price_index] == \"\" or len(article[self.rrp_price_index]) == 0 or article[\n",
    "            self.rrp_price_index] == \"0.00 €\" \\\n",
    "                or article[self.rrp_price_index] == \"0.00\":\n",
    "            article[self.rrp_price_index] = article[self.search_price_index]\n",
    "        if article[self.delivery_cost_index] == '\"0,00 EUR\"' or article[self.delivery_cost_index] == ''\"0.00 EUR\"'':\n",
    "            article[self.delivery_cost_index] = \"0\"\n",
    "        for euro_token in synonym_euro:\n",
    "            if euro_token in article[self.search_price_index]:\n",
    "                article[self.search_price_index]: str = article[self.search_price_index].replace(euro_token, \"\")\n",
    "            if euro_token in article[self.rrp_price_index]:\n",
    "                article[self.rrp_price_index]: str = article[self.rrp_price_index].replace(euro_token, \"\")\n",
    "            if euro_token in article[self.delivery_cost_index]:\n",
    "                article[self.delivery_cost_index]: str = article[self.delivery_cost_index].replace(euro_token, \"\")\n",
    "        article[self.search_price_index]: str = article[self.search_price_index].replace(\".\", \",\")\n",
    "        article[self.rrp_price_index]: str = article[self.rrp_price_index].replace(\".\", \",\")\n",
    "        article[self.delivery_cost_index]: str = article[self.delivery_cost_index].replace(\".\", \",\")\n",
    "        return article\n",
    "\n",
    "    def clean_prices(self, list_articles):\n",
    "        p = Pool()\n",
    "        cleaned_prices_articles = p.map(self.clean_price, list_articles)\n",
    "        return cleaned_prices_articles\n",
    "\n",
    "    def get_article_identifier(self, article: list) -> str:\n",
    "        \"\"\"\n",
    "        Get an article identifier in order to merge articles by their sizes\n",
    "        :return: identifier\n",
    "        \"\"\"\n",
    "\n",
    "        if \"Avocadostore\" in article[self.merchantName_index]:\n",
    "            merchant_product_id = article[self.merchant_product_id_index]\n",
    "            split_merchant_product_id_index = merchant_product_id.split(\"-\")\n",
    "            colour: str = article[self.colour_index]\n",
    "            product_identifier: str = split_merchant_product_id_index[0]\n",
    "            identifier: str = product_identifier + \"-\" + colour\n",
    "        else:\n",
    "            identifier: str = \"aw_image_url\"\n",
    "        return identifier\n",
    "\n",
    "    def merged_product_by_size(self, input_list_articles):\n",
    "        \"\"\"\n",
    "        Merge product by size, based on \"unique\" the aw_image_url.\n",
    "        :param input_list_articles: List of all articles with \"duplicates\" by size\n",
    "        :return: list_articles_merged\n",
    "        \"\"\"\n",
    "        headers = input_list_articles[0]\n",
    "        list_art = input_list_articles[1:]\n",
    "        list_articles_merged = []\n",
    "        mapping_identifier_sizes = defaultdict(list)\n",
    "        mapping_identifier_article = {}\n",
    "        mapping_column_header = get_mapping_column_index(self.input_data_feed, \"\\t\")\n",
    "\n",
    "        for article in list_art:\n",
    "\n",
    "            identifier_column = merchant_to_identifier.get(\"EMPTY\", article[self.column_id_mapping[\"merchant_name\"]])\n",
    "\n",
    "            size_content = article[mapping_column_header[\"Fashion:size\"]]\n",
    "            size_content = clean_size(size_content)\n",
    "            stock_content = article[mapping_column_header[\"stock_status\"]]\n",
    "\n",
    "            if \"Avocadostore\" in article[self.merchantName_index]:\n",
    "                merchant_product_id = article[self.merchant_product_id_index]\n",
    "                split_merchant_product_id_index = merchant_product_id.split(\"-\")\n",
    "                colour: str = article[self.colour_index]\n",
    "                product_identifier: str = split_merchant_product_id_index[0]\n",
    "                identifier: str = product_identifier + \"-\" + colour\n",
    "                mapping_identifier_sizes[identifier].append(\n",
    "                    size_content)  # Mapping URL sizes\n",
    "                mapping_identifier_article[identifier] = article  # Mapping URL article\n",
    "\n",
    "            else:\n",
    "                mapping_identifier_sizes[article[mapping_column_header[identifier_column]]].append(\n",
    "                    size_content)  # Mapping URL sizes\n",
    "                mapping_identifier_article[\n",
    "                    article[mapping_column_header[identifier_column]]] = article  # Mapping URL article\n",
    "\n",
    "        # Add the sizes columns to the headers\n",
    "        headers = [header.replace(\"Fashion:size\", \"Fashion:size0\") for header in headers]\n",
    "        for i in range(1, maxNumberFashionSizeColumns):  # Start at one because we already use Fashion:size0\n",
    "            headers.append(\"Fashion:size\" + str(i))\n",
    "\n",
    "        mapping_header_column_id = {header: columnId for columnId, header in enumerate(headers)}\n",
    "        # Put the size into the size column\n",
    "        for url, sizes in mapping_identifier_sizes.items():\n",
    "            list_size: list = []\n",
    "            for lt in sizes:\n",
    "                for size in lt:\n",
    "                    list_size.append(size)\n",
    "            article = mapping_identifier_article[url]\n",
    "            # Append empty rows for the new sizes column\n",
    "            for i in range(maxNumberFashionSizeColumns):\n",
    "                article.append(\"\")\n",
    "            list_size = list_size[:maxNumberFashionSizeColumns]\n",
    "            list_size = list(set(list_size))  # remove duplicates\n",
    "            size_sorter: SizeSorter = SizeSorter(list_size)\n",
    "            list_size: list = size_sorter.sorted_sizes\n",
    "            for i, size in enumerate(list_size):\n",
    "                article[mapping_header_column_id[\"Fashion:size\" + str(i)]] = size\n",
    "            list_articles_merged.append(article)\n",
    "\n",
    "        return [headers] + list_articles_merged\n",
    "\n",
    "    def predict_categories(self, list_articles: list, ) -> list:\n",
    "        \"\"\"\n",
    "        With a farm model we predict the categories of the articles based on relevant columns\n",
    "        :param list_articles: \n",
    "        :return: list_categories with a predicted category_name\n",
    "        \"\"\"\n",
    "\n",
    "        category_predicted_index = get_headers_index(\"category_predicted\")\n",
    "        list_articles_with_new_categories = []\n",
    "        list_column_features = self.column_features\n",
    "        headers = list_articles[0]\n",
    "        interesting_data = []\n",
    "        list_index_interesting_data = []\n",
    "        for i, header in enumerate(headers):\n",
    "            if header in list_column_features:\n",
    "                list_index_interesting_data.append(i)\n",
    "        list_articles = list_articles[1:]  # skip headers\n",
    "        for article in list_articles:\n",
    "            article_data = []\n",
    "            for position in list_index_interesting_data:\n",
    "                article_data.append(article[position])\n",
    "            article_data = \" [SEP] \".join(article_data)\n",
    "            interesting_data.append({\"text\": article_data})\n",
    "\n",
    "        interesting_data = interesting_data[1:]  # skip headers\n",
    "        model = Inferencer.load(self.model_path_categories, batch_size=batch_size, gpu=True,\n",
    "                                task_type=\"text_classification\",\n",
    "                                disable_tqdm=True, use_fast=True)\n",
    "        results = model.inference_from_dicts(dicts=interesting_data)\n",
    "        prediction_position = 0\n",
    "        for i, predictions in enumerate(results):\n",
    "            for prediction in predictions[\"predictions\"]:\n",
    "                prediction_position += 1\n",
    "                label = prediction[\"label\"]\n",
    "                list_articles[prediction_position][category_predicted_index] = label\n",
    "                list_articles_with_new_categories.append(list_articles[prediction_position])\n",
    "        return list_articles_with_new_categories  # Does not return headers\n",
    "\n",
    "    def predict_origin(self, list_articles: list, ) -> list:\n",
    "        \"\"\"\n",
    "        With a farm model we predict the categories of the articles based on relevant columns\n",
    "        :param list_articles:\n",
    "        :return: list_categories with a predicted category_name\n",
    "        \"\"\"\n",
    "\n",
    "        origin_predicted_index = column_index_mapping[\"origin_predicted\"]\n",
    "        data_to_predict = []\n",
    "        list_column_features = self.column_features_origin\n",
    "        headers = list_articles[0]\n",
    "        interesting_data = []\n",
    "        list_index_interesting_data = []\n",
    "        for i, header in enumerate(headers):\n",
    "            if header in list_column_features:\n",
    "                list_index_interesting_data.append(i)\n",
    "        list_articles = list_articles[1:]  # skip headers\n",
    "        for article in list_articles:\n",
    "            article_data = []\n",
    "            for position in list_index_interesting_data:\n",
    "                article_data.append(article[position])\n",
    "            article_data = \" [SEP] \".join(article_data)\n",
    "            interesting_data.append({\"text\": article_data})\n",
    "\n",
    "        interesting_data = interesting_data[1:]  # skip headers\n",
    "        model = Inferencer.load(self.model_path_origin, batch_size=batch_size, gpu=True,\n",
    "                                task_type=\"text_classification\",\n",
    "                                disable_tqdm=True, use_fast=True)\n",
    "        results = model.inference_from_dicts(dicts=interesting_data)\n",
    "        prediction_position = 0\n",
    "        for i, predictions in enumerate(results):\n",
    "            for prediction in predictions[\"predictions\"]:\n",
    "                prediction_position += 1\n",
    "                label = prediction[\"label\"]\n",
    "                label = label.replace('\"', \"\")\n",
    "                label = label.replace(\"[\", \"\")\n",
    "                label = label.replace(\"]\", \"\")\n",
    "                label = label.replace(\"'\", \"\")\n",
    "                label = label.replace(\" \", \"\")\n",
    "                labels = label.split(\",\")\n",
    "                labels = labels[0:3]  # take the first 3 colors\n",
    "\n",
    "                list_articles[prediction_position][origin_predicted_index] = labels[0]\n",
    "        return list_articles\n",
    "\n",
    "    def predict_colors(self, list_articles: list) -> list:\n",
    "        color_index = get_headers_index(\"colour\")\n",
    "        colors_normalized_zero_index = get_headers_index(\"color_normalized_0\")\n",
    "        colors_normalized_one_index = get_headers_index(\"color_normalized_1\")\n",
    "        colors_normalized_two_index = get_headers_index(\"color_normalized_2\")\n",
    "\n",
    "        data_to_predict = []\n",
    "        list_articles_with_new_colors = []\n",
    "        for article in list_articles:\n",
    "            if len(article[color_index]) == 0:\n",
    "                color_text = \"NO_COLOR\"\n",
    "            else:\n",
    "                color_text = article[color_index]\n",
    "            data_to_predict.append({\"text\": color_text})\n",
    "\n",
    "        data_to_predict = data_to_predict[1:]  # skip headers\n",
    "        model = Inferencer.load(self.model_path_colors, batch_size=batch_size * 2, gpu=True,\n",
    "                                task_type=\"text_classification\",\n",
    "                                disable_tqdm=True, use_fast=True)\n",
    "        results = model.inference_from_dicts(dicts=data_to_predict)\n",
    "        prediction_position = 0\n",
    "        for i, predictions in enumerate(results):\n",
    "            for prediction in predictions[\"predictions\"]:\n",
    "                prediction_position += 1\n",
    "                label = prediction[\"label\"]\n",
    "                label = label.replace('\"', \"\")\n",
    "                label = label.replace(\"[\", \"\")\n",
    "                label = label.replace(\"]\", \"\")\n",
    "                label = label.replace(\"'\", \"\")\n",
    "                label = label.replace(\" \", \"\")\n",
    "                labels = label.split(\",\")\n",
    "                labels = labels[0:3]  # take the first 3 colors\n",
    "\n",
    "                list_articles[prediction_position][colors_normalized_zero_index] = labels[0]\n",
    "                if len(labels) == 2:\n",
    "                    list_articles[prediction_position][colors_normalized_one_index] = labels[1]\n",
    "                if len(labels) == 3:\n",
    "                    list_articles[prediction_position][colors_normalized_two_index] = labels[2]\n",
    "                list_articles_with_new_colors.append(list_articles[prediction_position])\n",
    "        return list_articles_with_new_colors\n",
    "\n",
    "    def predict_saison(self, list_articles: list) -> list:\n",
    "        saison_index = get_headers_index(\"saison\")\n",
    "        saison_conf_score_index = get_headers_index(\"saison_conf_score_index\")\n",
    "\n",
    "        list_articles_with_saison = []\n",
    "        list_column_features = self.column_features\n",
    "        headers = list_articles[0]\n",
    "        interesting_data = []\n",
    "        list_index_interesting_data = []\n",
    "        for i, header in enumerate(headers):\n",
    "            if header in list_column_features:\n",
    "                list_index_interesting_data.append(i)\n",
    "        list_articles = list_articles[1:]  # skip headers\n",
    "\n",
    "        for article in list_articles:\n",
    "            article_data = []\n",
    "            for position in list_index_interesting_data:\n",
    "                article_data.append(article[position])\n",
    "            article_data = \" [SEP] \".join(article_data)\n",
    "\n",
    "            interesting_data.append({\"text\": article_data})\n",
    "        interesting_data = interesting_data[1:]  # skip headers\n",
    "        model = Inferencer.load(self.model_path_saison, batch_size=batch_size, gpu=True,\n",
    "                                task_type=\"text_classification\",\n",
    "                                disable_tqdm=True, use_fast=True)\n",
    "        results = model.inference_from_dicts(dicts=interesting_data)\n",
    "        prediction_position = 0\n",
    "        for i, predictions in enumerate(results):\n",
    "            for prediction in predictions[\"predictions\"]:\n",
    "                prediction_position += 1\n",
    "                label = prediction[\"label\"]\n",
    "                label = label.replace('\"', \"\")\n",
    "                label = label.replace(\"[\", \"\")\n",
    "                label = label.replace(\"]\", \"\")\n",
    "                label = label.replace(\"'\", \"\")\n",
    "                label = label.replace(\" \", \"\")\n",
    "                labels = label.split(\",\")\n",
    "                labels = \",\".join(labels)\n",
    "                probability = prediction[\"probability\"]\n",
    "                list_articles[prediction_position][saison_conf_score_index] = probability\n",
    "                list_articles[prediction_position][saison_index] = labels\n",
    "                list_articles_with_saison.append(list_articles[prediction_position])\n",
    "        return list_articles_with_saison  # Does not return headers\n",
    "\n",
    "    def predict_keywords(self, list_articles: list) -> list:\n",
    "        keyword_index = get_headers_index(\"keywords\")\n",
    "        hard_keywords = [\"kunstleder\", \"lederoptik\", \"lederimitat\"]\n",
    "\n",
    "        list_articles_with_keyword = []\n",
    "        list_column_features = self.column_features_keywords\n",
    "        headers = list_articles[0]\n",
    "        interesting_data = []\n",
    "        list_index_interesting_data = []\n",
    "        for i, header in enumerate(headers):\n",
    "            if header in list_column_features:\n",
    "                list_index_interesting_data.append(i)\n",
    "\n",
    "        for i, article in enumerate(list_articles):\n",
    "            labels = set()\n",
    "            for hard_keyword in hard_keywords:\n",
    "                if hard_keyword in article[self.column_id_mapping[\"Title\"]].lower():\n",
    "                    list_articles[i][self.keyword_index] = \"Kunstleder\"\n",
    "\n",
    "        return list_articles\n",
    "\n",
    "    def add_item_id(self, article: list):\n",
    "        \"\"\"\n",
    "        Generate and add to the item_id column an item_id for every article\n",
    "        :param article:\n",
    "        :return: article\n",
    "        \"\"\"\n",
    "        article[self.item_id_index] = self.aw_deep_link_index\n",
    "\n",
    "    def ean_cleanser(self, list_articles: list) -> list:\n",
    "        \"\"\"\n",
    "\n",
    "        :param list_articles:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # if list only avocado store keep all\n",
    "        list_clean_ean_articles = []\n",
    "        list_index_articles_cleansed = []\n",
    "        list_articles_to_return = []\n",
    "        articles_index_to_return = []\n",
    "        merchant_ean_to_clean = [\"0Avocadostore\", \"1Im walking\", \"2mirapodo\", \"3OTTO\"]\n",
    "        ean_mapping = defaultdict(list)\n",
    "        ean_relevant_merchant = ranking_merchant_ean.keys()\n",
    "        ean_index = get_headers_index(\"ean\", self.input_data_feed)\n",
    "        merchant_name_index = self.merchantName_index\n",
    "        for a, article in enumerate(list_articles):\n",
    "            ean = article[ean_index]\n",
    "            merchant_name = article[self.merchantName_index]\n",
    "            if merchant_name == \"ETHLETIC\":\n",
    "                if ean.endswith(\".0\"):\n",
    "                    ean = ean[:-2]\n",
    "            ean_mapping[ean].append({\"merchant_name\": replace_merchant_names_ean_order(article[merchant_name_index]),\n",
    "                                     \"index\": a})\n",
    "            list_index_articles_cleansed.append(a)\n",
    "\n",
    "        for ean, merchants in ean_mapping.items():\n",
    "            ean_cleansed = False\n",
    "            if ean == str(889556801404):\n",
    "                print(\"xx\", merchants)\n",
    "            length_merchant_names = len(merchants)\n",
    "            list_merchant_names = [merchant[\"merchant_name\"] for merchant in merchants]\n",
    "            set_merchant_names = set(list_merchant_names)\n",
    "            # print(list_merchant_names)\n",
    "\n",
    "            if ean != \"\":\n",
    "                if len(set(list_merchant_names).intersection(merchant_ean_to_clean)) > 0:\n",
    "                    if \"0Avocadostore\" in list_merchant_names:\n",
    "                        if len(set_merchant_names) == 1:\n",
    "                            for merchant in merchants:\n",
    "                                articles_index_to_return.append(merchant[\"index\"])\n",
    "                        else:\n",
    "                            for merchant in merchants:\n",
    "                                if merchant[\"merchant_name\"] == \"0Avocadostore\":\n",
    "                                    articles_index_to_return.append(merchant[\"index\"])\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        ordered_merchants = []\n",
    "                        ord_merchants = {}\n",
    "                        ordered_merchants = [merchant[\"merchant_name\"] + \"-\" + str(merchant[\"index\"]) for merchant in\n",
    "                                             merchants]\n",
    "                        merchant_to_return = ordered_merchants[0]\n",
    "                        index_to_return = merchant_to_return.split(\"-\")[-1]\n",
    "                        index_to_return = int(index_to_return)\n",
    "                        articles_index_to_return.append(index_to_return)\n",
    "\n",
    "                else:  # length of merchant should be one\n",
    "                    articles_index_to_return.append(merchants[0][\"index\"])\n",
    "\n",
    "\n",
    "            else:\n",
    "                for merchant in merchants:\n",
    "                    articles_index_to_return.append(merchant[\"index\"])\n",
    "        for article_index in articles_index_to_return:\n",
    "            if type(article_index) == int:\n",
    "                list_articles_to_return.append(list_articles[article_index])\n",
    "\n",
    "        return list_articles_to_return\n",
    "\n",
    "    def articles_cleansing(self,list_articles):\n",
    "        with Pool() as p:\n",
    "            x = list(tqdm.tqdm(p.imap(self.article_cleansing, list_articles),\n",
    "                           total=len(list_articles)))\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool()\n",
    "\n",
    "#def cleansing():\n",
    "    #with Pool() as p:\n",
    "    cleanser = Cleanser()\n",
    "\n",
    "    print(\"Begin cleansing\", datetime.datetime.now())\n",
    "    list_articles = get_lines_csv(cleanser.input_data_feed, \"\\t\")\n",
    "    print(\"0\", len(list_articles))\n",
    "\n",
    "    print(\"Cleansing - Merging by size: Begin\", datetime.datetime.now())\n",
    "    list_articles = cleanser.merged_product_by_size(list_articles)\n",
    "    print(\"00\", len(list_articles))\n",
    "\n",
    "    print(\"Cleansing - Merging by size: Done\", datetime.datetime.now())\n",
    "    headers = list_articles[0]\n",
    "    list_articles = list_articles[1:]\n",
    "    print(\"Cleansing - Renaming Categories: Begin\", datetime.datetime.now())\n",
    "    list_articles = list_articles\n",
    "    list_articles = list(tqdm.tqdm(p.imap(cleanser.article_cleansing, list_articles),\n",
    "                                   total=len(list_articles)))\n",
    "\n",
    "    list_articles = list_articles\n",
    "    print(\"1\", len(list_articles))\n",
    "    # renaming article's category and fashion suitable for\n",
    "\n",
    "    list_articles = list(tqdm.tqdm(p.imap(cleanser.article_cleansing, list_articles),\n",
    "                                   total=len(list_articles)))\n",
    "    list_articles = cleanser.ean_cleanser(list_articles)\n",
    "\n",
    "    print(\"Cleansing - Renaming Categories: Done\", datetime.datetime.now())\n",
    "\n",
    "    print(\"Cleansing - Renaming Categories DL: Begin\", datetime.datetime.now())\n",
    "    list_articles = cleanser.predict_categories([headers] + list_articles)\n",
    "\n",
    "    print(\"Cleansing - Renaming Categories DL: Done\", datetime.datetime.now())\n",
    "\n",
    "    print(\"Cleansing - Renaming Colors DL: Begin\", datetime.datetime.now())\n",
    "    list_articles = cleanser.predict_colors(list_articles)\n",
    "\n",
    "    print(\"Cleansing - Renaming Colors DL: Done\", datetime.datetime.now())\n",
    "    print(\"Cleansing - Adding saison DL: Begin\", datetime.datetime.now())\n",
    "    list_articles = cleanser.predict_saison([headers] + list_articles)\n",
    "    print(\"Cleansing - Adding saison DL: Done\", datetime.datetime.now())\n",
    "\n",
    "    print(\"Cleansing - Adding keywords DL: Begin\", datetime.datetime.now())\n",
    "\n",
    "    list_articles = cleanser.predict_keywords([headers] + list_articles)\n",
    "    print(\"Cleansing - Adding keywords DL: Done\", datetime.datetime.now())\n",
    "\n",
    "    print(\"Cleansing - Adding origin DL: Begin\", datetime.datetime.now())\n",
    "    # list_articles = cleanser.predict_origin([headers] + list_articles)\n",
    "    print(\"Cleansing - Adding origin DL: Done\", datetime.datetime.now())\n",
    "\n",
    "    print(\"Cleansing - Sexes and Prices: Begin\")\n",
    "    cleansed_fashion_suitable_for = list(\n",
    "        tqdm.tqdm(p.imap(cleanser.renaming_fashion_suitable_for, list_articles),\n",
    "                  total=(len(list_articles)), desc=\"Cleansing Fashion Suitable for\"))\n",
    "\n",
    "    cleansed_prices = list(tqdm.tqdm(p.map(cleanser.clean_price,\n",
    "                                           cleansed_fashion_suitable_for), total=len(cleansed_fashion_suitable_for),\n",
    "                                     desc=\"Cleansing Prices\"))\n",
    "    print(\"Cleansing - Sexes and Prices: Done\", datetime.datetime.now())\n",
    "    cleansed_articles = [headers] + cleansed_prices\n",
    "    write_2_file(cleansed_articles, file_paths[\"cleansed_sex_data_feed_path\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
